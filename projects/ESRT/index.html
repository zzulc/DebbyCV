<!DOCTYPE html>
<html>
<head>
<title>CVPRW_ESRT</title>

<style media="screen" type="text/css">
body
{
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1080px;
  font-family: Times New Roman, Lato, Verdana, Helvetica, sans-serif;
  font-size: 14px;
  background: #fdfdfd;
}
</style>

<script>
var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?1ec4ad5c61857459aa78d5ee7ddee28d";
  var s = document.getElementsByTagName("script")[0];
  s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

<body>
<h3 align="center"><i><font size="3" face="Palatino Linotype">The Conference on Computer Vision and Pattern Recognition Workshop (NTIRE) 2022</font></i></h3>

<table align="center">
<td align="center">
<h1>Transformer for Single Image Super-Resolution</h1>
<h3>
	<font size="3"><b>Zhisheng Lu</b></font></a><sup><font size="2">1#</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<a href="https://junchenglee.com/" target="_blank"><font size="3"><b>Juncheng Li</b></font></a><sup><font size="2">2#</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3">Hong Liu</font><sup><font size="2">1*</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3">Chaoyan Huang</font><sup><font size="2">3</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3">Linlin Zhang</font><sup><font size="2">1</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
	<font size="3">Tieyong Zeng</font><sup><font size="2">2</font></sup>&nbsp;&nbsp;&nbsp;&nbsp;
</h3>

<sup><font size="2">1</font></sup>
<b><a><font size="3">Peking University Shenzhen Graduate School</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;
<sup><font size="2">2</font></sup>
<b><a><font size="3">The Chinese University of Hong Kong</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;
<sup><font size="2">3</font></sup>
<b><a><font size="3">Nanjing University of Posts and Telecommunications</font></a></b>&nbsp;&nbsp;&nbsp;&nbsp;

<br>
<br>&nbsp;
	<b><a><font size="3"> #Co-ﬁrst authors, *Corresponding author &nbsp;&nbsp;&nbsp;&nbsp; Contact us: cvjunchengli@gmail.con; zhisheng_lu@pku.edu.cn</font></a></b>
<br>

</td>
</table>


<br><br>
<table align="center">
<tr>
	<td align="center"><embed src="head.png" width="600"></td>
</tr>
</table>


<br>
<h2><p><font size="6"><b>Abstract</b></font></p></h2>
<hr/>
<p><font size="4" face="Palatino Linotype">Single image super-resolution (SISR) has witnessed great strides with the development of deep learning. However, most existing studies focus on building more complex networks with a massive number of layers. Recently, more and more researchers start to explore the application of Transformer in computer vision tasks. However, the heavy computational cost and high GPU memory occupation of the vision Transformer cannot be ignored. In this paper, we propose a novel Efﬁcient Super-Resolution Transformer (ESRT) for SISR. ESRT is a hybrid model, which consists of a Lightweight CNN Backbone (LCB) and a Lightweight Transformer Backbone (LTB). Among them, LCB can dynamically adjust the size of the feature map to extract deep features with a low computational costs. LTB is composed of a series of Efﬁcient Transformers (ET), which occupies a small GPU memory occupation, thanks to the specially designed Efﬁcient Multi-Head Attention (EMHA). Extensive experiments show that ESRT achieves competitive results with low computational cost. Compared with the original Transformer which occupies 16,057M GPU memory, ESRT only occupies 4,191M GPU memory.
</font></p>


<br>
<h2><p><font size="6"><b>Motivation</b></font></p></h2>
<hr/>
<table align="center">
</table>
<p><font size="4" face="Palatino Linotype">As shown in the figure below, the inner areas of the boxes with the same color are similar to each other. Therefore, these similar image patches can be used as reference images for each other, so that the texture details of the certain patch can be restored with reference patches. Inspired by this, we introduce the Transformer into the SISR task since it has a strong feature expression ability to model such a long-term dependency in the image. In recent years, some Vision-Transformer have been proposed for computer vision tasks. However, these methods often occupy heavy GPU memory, which greatly limits their ﬂexibility and application scenarios. Moreover, these methods cannot be directly transferred to SISR since the image restoration task often take a larger resolution image as input, which will take up huge memory. To solve this, we aim to explore a more efﬁcient vision-Transformer for SISR.
</font></p>
<table align="center">
<tr>
	<td align="center"><img border=0 width=600 src="EP.png"></td>
</tr>
<tr>
<td align="center">Examples of similar patches in images. These similar patches can help restore details from each other.</td>
</tr>
</table>


<br>
<h2><p><font size="6"><b>ESRT</b></font></p></h2>
<table align="center">
<hr/>
<tr>
	<td align="center"><img border=0 width=900 src="ESRT.png"></td>
</tr>
<tr>
	<td align="center">The architecture of the proposed Efﬁcient Super-Resolution Transformer.</td>
</tr>
</table>


<table align="center">
<hr/>
<tr>
	<td align="center"><img border=0 width=900 src="processing.png"></td>
</tr>
<tr>
	<td align="center">The complete pre- and post-processing for the Efﬁcient Transformer (ET).</td>
</tr>
</table>


<table align="center">
<hr/>
<tr>
	<td align="center"><img border=0 width=700 src="EMHA.png"></td>
</tr>
<tr>
	<td align="center">Architecture of Efﬁcient Transformer. EMHA is the Efﬁcient Multi-Head Attention. MatMul is the matrix multiplication.</td>
</tr>
</table>




<br>
<h2><p><font size="6"><b>Visual Results</b></font></p></h2>
<hr/>
<table align="center">
<tr>
	<td align="center"><img border=0 width=900 src="Result2.png"></td>
</tr>
</table>




<br>
<h2><p><font size="6"><b>PSNR/SSIM Results</b></font></p></h2>
<hr/>
<table align="center">
<tr>
	<td align="center"><img border=0 width=900 src="Result1.png"></td>
</tr>
</table>



<br>
<h2><p><font size="6"><b>Downloads</b></font></p></h2>
<hr/>
<div align="left">
		<table>						
		<tr align="left">
		<td>
			<font size="4">Paper</font>
		</td>
		<td>
			<font size="4">: <a href="https://arxiv.org/pdf/2108.11084.pdf" target="_blank">[ Paper ]</a></font>
		</td>
		</tr>
			
       <tr align="left">
		<td>
			<font size="4">Supp Material</font>
		</td>
		<td>
			<font size="4">: <a href="https://junchenglee.com/paper/ESRT-supp.pdf" target="_blank">[ Supplementary Material ]</a></font>
		</td>
	    </tr>
					
		<tr align="left">
		<td>
			<font size="4">Source Code</font>
		</td>
		<td>
			<font size="4">: <a href="https://github.com/luissen/ESRT" target="_blank">[ Code ]</a> </font>
		</td>
		</tr>	

		<tr align="left">
		<td>
			<font size="4">ESRT Result</font>
		</td>
		<td>
			<font size="4">: <a href="https://www.jianguoyun.com/p/DZv442AQ19ySBxjcsbsEIAA" target="_blank">[ Reconstruction Results ]</a> </font>
		</td>
		</tr>	
			
			
		</table>
</div>


<h2><p><font size="6" color="black"><b>Statement</b></font></p></h2>
<hr/>
<font size="3">
The original title of this paper was "Efficient Transformer for Single Image Super-Resolution". </br>
The final version of this paper is "Transformer for Single Image Super-Resolution". 
</font>


<h2><p><font size="6" color="black"><b>BibTex</b></font></p></h2>
<hr/>
<font size="3">
@InProceedings{fang2020multi,<br>
&nbsp;&nbsp;&nbsp;&nbsp;title = {Transformer for Single Image Super-Resolution},<br>
&nbsp;&nbsp;&nbsp;&nbsp;author = {Lu Zhisheng, Li Juncheng, Liu Hong, Huang Chaoyan, Zhang Linlin, and Zeng Tieyong},<br>
&nbsp;&nbsp;&nbsp;&nbsp;booktitle = {ICVPRW},<br>
&nbsp;&nbsp;&nbsp;&nbsp;year = {2022},<br>
}
</font>


</body>

</html>
